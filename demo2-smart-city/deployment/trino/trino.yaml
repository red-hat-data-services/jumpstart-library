apiVersion: v1
data:
  hive-exec-log4j2.properties: |
    status = INFO
    name = HiveLog4j2
    packages = org.apache.hadoop.hive.ql.log

    # list of properties
    property.hive.log.level = INFO
    property.hive.root.logger = console
    property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
    property.hive.log.file = hive.log

    # list of all appenders
    appenders = console

    # console appender
    appender.console.type = Console
    appender.console.name = console
    appender.console.target = SYSTEM_ERR
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n

    # list of all loggers
    loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX

    logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
    logger.NIOServerCnxn.level = WARN

    logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
    logger.ClientCnxnSocketNIO.level = WARN

    logger.DataNucleus.name = DataNucleus
    logger.DataNucleus.level = ERROR

    logger.Datastore.name = Datastore
    logger.Datastore.level = ERROR

    logger.JPOX.name = JPOX
    logger.JPOX.level = ERROR

    # root logger
    rootLogger.level = ${sys:hive.log.level}
    rootLogger.appenderRefs = root
    rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
  hive-log4j2.properties: |
    status = INFO
    name = HiveLog4j2
    packages = org.apache.hadoop.hive.ql.log

    # list of properties
    property.hive.log.level = INFO
    property.hive.root.logger = console
    property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
    property.hive.log.file = hive.log

    # list of all appenders
    appenders = console

    # console appender
    appender.console.type = Console
    appender.console.name = console
    appender.console.target = SYSTEM_ERR
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n

    # list of all loggers
    loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX

    logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
    logger.NIOServerCnxn.level = WARN

    logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
    logger.ClientCnxnSocketNIO.level = WARN

    logger.DataNucleus.name = DataNucleus
    logger.DataNucleus.level = ERROR

    logger.Datastore.name = Datastore
    logger.Datastore.level = ERROR

    logger.JPOX.name = JPOX
    logger.JPOX.level = ERROR

    # root logger
    rootLogger.level = ${sys:hive.log.level}
    rootLogger.appenderRefs = root
    rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
  hive-site.xml: "<configuration>\n  <property>\n    <name>hive.server2.enable.doAs</name>\n
    \   <value>false</value>\n  </property>\n  <property>\n    <name>hive.server2.use.SSL</name>\n
    \   <value>false</value>\n  </property>\n  <property>\n    <name>hive.server2.authentication</name>\n
    \   <value>NOSASL</value>\n  </property>\n  <property>\n    <name>hive.metastore.metrics.enabled</name>\n
    \   <value>true</value>\n  </property>\n  <property>\n    <name>hive.server2.metrics.enabled</name>\n
    \   <value>true</value>\n  </property>\n  <property>\n    <name>hive.service.metrics.reporter</name>\n
    \   <value>JMX</value>\n  </property>\n  <property>\n    <name>hive.server2.thrift.bind.host</name>\n
    \   <value>0.0.0.0</value>\n  </property>\n  <property>\n    <name>hive.metastore.thrift.bind.host</name>\n
    \   <value>0.0.0.0</value>\n  </property>\n  <property>\n    <name>hive.metastore.uris</name>\n
    \   <value>thrift://hive-metastore-0:9083,thrift://hive-metastore-1:9083,thrift://hive-metastore-2:9083</value>\n
    \ </property>\n  <property>\n    <name>datanucleus.schema.autoCreateAll</name>\n
    \   <value>false</value>\n  </property>\n  <property>\n    <name>hive.metastore.schema.verification</name>\n
    \   <value>false</value>\n  </property>\n  <property>\n    <name>hive.default.fileformat</name>\n
    \   <value>Parquet</value>\n  </property>\n  <property>\n    <name>hive.metastore.warehouse.dir</name>\n
    \   <value>s3a://XXX_S3ENDPOINT_XXX/</value>\n  </property>\n        <property>
    <name>fs.s3a.endpoint</name>\n    <value>XXX_S3ENDPOINT_URL_PREFIX_XXXXXX_S3LOCATION_XXX/</value>\n
    \ </property>\n <property> \n  <name>hive.metastore.db.type</name>\n  <value>POSTGRES</value>\n
    \ <description>\n    Expects one of [derby, oracle, mysql, mssql, postgres].\n
    \   Type of database used by the metastore. Information schema &amp; JDBCStorageHandler
    depend on it.\n  </description>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionUserName</name>\n
    \   <value>XXX_DATABASE_USER_XXX</value>\n    <description>Username to use against
    metastore database</description>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionPassword</name>\n
    \   <value>XXX_DATABASE_PASSWORD_XXX</value>\n    <description>password to use
    against metastore database</description>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionURL</name>\n
    \   <value>jdbc:XXX_DATABASE_CONNECT_URL_XXX</value>\n    <description>\n      JDBC
    connect string for a JDBC metastore.\n      To use SSL to encrypt/authenticate
    the connection, provide database-specific SSL flag in the connection URL.\n      For
    example, jdbc:postgresql://myhost/db?ssl=true for postgres database.\n    </description>\n
    \ </property>\n  <property>\n    <name>javax.jdo.option.ConnectionDriverName</name>\n
    \   <value>org.postgresql.Driver</value>\n    <description>Driver class name for
    a JDBC metastore</description>\n  </property>\n  <property>\n    <name>hive.cluster.delegation.token.store.class</name>\n
    \   <value>org.apache.hadoop.hive.thrift.DBTokenStore</value>\n  </property>\n</configuration>\n"
kind: ConfigMap
metadata:
  name: hive-config
  namespace: smartcity
---
apiVersion: v1
data:
  config.yml: |
    ---
    lowercaseOutputName: true
    lowercaseOutputLabelNames: true
    attrNameSnakeCase: true
    whitelistObjectNames:
      - 'metrics:name=active_calls_*'
      - 'metrics:name=api_*'
      - 'metrics:name=create_*'
      - 'metrics:name=delete_*'
      - 'metrics:name=init_*'
      - 'metrics:name=exec_*'
      - 'metrics:name=hs2_*'
      - 'metrics:name=open_connections'
      - 'metrics:name=open_operations'
    rules:
      - pattern: 'metrics<name=(.*)><>Value'
        name: hive_$1
        type: GAUGE
      - pattern: 'metrics<name=(.*)><>Count'
        name: hive_$1_count
        type: GAUGE
      - pattern: 'metrics<name=(.*)><>(\d+)thPercentile'
        name: hive_$1
        type: GAUGE
        labels:
          quantile: "0.$2"
kind: ConfigMap
metadata:
  labels:
    app: hive
  name: hive-jmx-config
  namespace: smartcity
---
apiVersion: v1
data:
  entrypoint.sh: |
    #!/bin/bash
    function importCert() {
      PEM_FILE=$1
      PASSWORD=$2
      KEYSTORE=$3
      # number of certs in the PEM file
      CERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)

      # For every cert in the PEM file, extract it and import into the JKS keystore
      # awk command: step 1, if line is in the desired cert, print the line
      #              step 2, increment counter when last line of cert is found
      for N in $(seq 0 $(($CERTS - 1))); do
        ALIAS="${PEM_FILE%.*}-$N"
        cat $PEM_FILE |
          awk "n==$N { print }; /END CERTIFICATE/ { n++ }" |
          keytool -noprompt -import -trustcacerts \
                  -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD
      done
    }
    set -e

    # if the s3-compatible ca bundle is mounted in, add to the root Java truststore.
    if [ -a /s3-compatible-ca/ca-bundle.crt ]; then
      echo "Adding /s3-compatible-ca/ca-bundle.crt to $JAVA_HOME/lib/security/cacerts"
      importCert /s3-compatible-ca/ca-bundle.crt changeit $JAVA_HOME/lib/security/cacerts
    fi
    # always add the openshift service-ca.crt if it exists
    if [ -a /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt ]; then
      echo "Adding /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt to $JAVA_HOME/lib/security/cacerts"
      importCert /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt changeit $JAVA_HOME/lib/security/cacerts
    fi

    # add UID to /etc/passwd if missing
    if ! whoami &> /dev/null; then
        if test -w /etc/passwd || stat -c "%a" /etc/passwd | grep -qE '.[267].'; then
            echo "Adding user ${USER_NAME:-hadoop} with current UID $(id -u) to /etc/passwd"
            # Remove existing entry with user first.
            # cannot use sed -i because we do not have permission to write new
            # files into /etc
            sed  "/${USER_NAME:-hadoop}:x/d" /etc/passwd > /tmp/passwd
            # add our user with our current user ID into passwd
            echo "${USER_NAME:-hadoop}:x:$(id -u):0:${USER_NAME:-hadoop} user:${HOME}:/sbin/nologin" >> /tmp/passwd
            # overwrite existing contents with new contents (cannot replace the
            # file due to permissions)
            cat /tmp/passwd > /etc/passwd
            rm /tmp/passwd
        fi
    fi

    # symlink our configuration files to the correct location
    if [ -f /hadoop-config/core-site.xml ]; then
      cat /hadoop-config/core-site.xml | sed "s#XXX_S3ENDPOINT_XXX#${S3_BUCKET_NAME}/${S3_DATA_DIR}#" > /opt/hadoop/etc/hadoop/core-site.xml
    else
      echo "core-site.xml doesnt exist, skipping"
    fi
    if [ -f /hadoop-config/hdfs-site.xml ]; then
      ln -s -f /hadoop-config/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml
    else
      echo "hdfs-site.xml doesnt exist, skipping symlink"
    fi

    # insert S3 bucket URI from env vars
    cat /hive-config/hive-site.xml | sed \
      -e "s#XXX_S3ENDPOINT_URL_PREFIX_XXX#${S3_ENDPOINT_URL_PREFIX}#" \
      -e "s#XXX_S3LOCATION_XXX#${S3_ENDPOINT}#" \
      -e "s#XXX_S3ENDPOINT_XXX#${S3_BUCKET_NAME}/${S3_DATA_DIR}#" \
      -e "s#XXX_DATABASE_USER_XXX#${DATABASE_USER}#" \
      -e "s#XXX_DATABASE_PASSWORD_XXX#${DATABASE_PASSWORD}#" \
      -e "s#XXX_DATABASE_CONNECT_URL_XXX#postgresql://${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_NAME}?sslmode=${DATABASE_SSLMODE}#" \
    > $HIVE_HOME/conf/hive-site.xml

    ln -s -f /hive-config/hive-log4j2.properties $HIVE_HOME/conf/hive-log4j2.properties
    ln -s -f /hive-config/hive-exec-log4j2.properties $HIVE_HOME/conf/hive-exec-log4j2.properties

    export HADOOP_LOG_DIR="${HADOOP_HOME}/logs"
    # Set garbage collection settings
    export GC_SETTINGS="-XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${HADOOP_LOG_DIR}/heap_dump.bin -XX:+ExitOnOutOfMemoryError -XX:ErrorFile=${HADOOP_LOG_DIR}/java_error%p.log"
    export VM_OPTIONS="$VM_OPTIONS -XX:+UseContainerSupport"

    if [ -n "$JVM_INITIAL_RAM_PERCENTAGE" ]; then
      VM_OPTIONS="$VM_OPTIONS -XX:InitialRAMPercentage=$JVM_INITIAL_RAM_PERCENTAGE"
    fi
    if [ -n "$JVM_MAX_RAM_PERCENTAGE" ]; then
      VM_OPTIONS="$VM_OPTIONS -XX:MaxRAMPercentage=$JVM_MAX_RAM_PERCENTAGE"
    fi
    if [ -n "$JVM_MIN_RAM_PERCENTAGE" ]; then
      VM_OPTIONS="$VM_OPTIONS -XX:MinRAMPercentage=$JVM_MIN_RAM_PERCENTAGE"
    fi

    # Set JMX options
    export JMX_OPTIONS="-javaagent:/opt/jmx_exporter/jmx_exporter.jar=8082:/opt/jmx_exporter/config/config.yml -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=8081 -Dcom.sun.management.jmxremote.rmi.port=8081 -Djava.rmi.server.hostname=127.0.0.1"

    # Set garbage collection logs
    export GC_SETTINGS="${GC_SETTINGS} -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc.log"
    export GC_SETTINGS="${GC_SETTINGS} -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=3M"

    export HIVE_LOGLEVEL="${HIVE_LOGLEVEL:-INFO}"
    export HADOOP_OPTS="${HADOOP_OPTS} ${VM_OPTIONS} ${GC_SETTINGS} ${JMX_OPTIONS}"
    export HIVE_METASTORE_HADOOP_OPTS=" -Dhive.log.level=${HIVE_LOGLEVEL} "
    export HIVE_OPTS="${HIVE_OPTS} --hiveconf hive.root.logger=${HIVE_LOGLEVEL},console "

    set +e
    if schematool -dbType postgres -info -verbose; then
        echo "Hive metastore schema verified."
    else
        if schematool -dbType postgres -initSchema -verbose; then
            echo "Hive metastore schema created."
        else
            echo "Error creating hive metastore: $?"
        fi
    fi
    set -e

    exec $@
kind: ConfigMap
metadata:
  name: hive-scripts
  namespace: smartcity
---
apiVersion: v1
data:
  hive_metastore_cpu_limit: "1"
  hive_metastore_cpu_request: 300m
  hive_metastore_memory_limit: 1Gi
  hive_metastore_memory_request: 1Gi
  s3_bucket: secor-obc
  s3_credentials_secret: secor-obc
  s3_endpoint_url: s3.data.local
  s3_endpoint_url_prefix: http://
  storage_class: ocs-storagecluster-ceph-rbd
  trino_cpu_limit: "1"
  trino_cpu_request: 300m
  trino_db_secret: trino-db-secret
  trino_environment: trino
  trino_image: trinodb/trino:403
  trino_memory_limit: 4Gi
  trino_memory_request: 4Gi
kind: ConfigMap
metadata:
  name: trino-config
  namespace: smartcity
---
apiVersion: v1
kind: Secret
metadata:
  name: aws-secret
  namespace: smartcity
stringData:
  AWS_ACCESS_KEY_ID: changeme
  AWS_SECRET_ACCESS_KEY: changeme
  BUCKET: changeme
---
apiVersion: v1
kind: Secret
metadata:
  name: hadoop-config
  namespace: smartcity
stringData:
  core-site.xml: |
    <configuration>
      <property>
          <name>fs.defaultFS</name>
          <value>s3a://XXX_S3ENDPOINT_XXX/</value>
      </property>
      <property>
          <name>fs.gs.impl</name>
          <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
      </property>
      <property>
          <name>fs.AbstractFileSystem.wasb.Impl</name>
          <value>org.apache.hadoop.fs.azure.Wasb</value>
      </property>
      <property>
          <name>fs.AbstractFileSystem.gs.impl</name>
          <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>
      </property>
      <property>
          <name>fs.gs.auth.service.account.enable</name>
          <value>true</value>
      </property>
      <property>
          <name>fs.gs.reported.permissions</name>
          <value>733</value>
      </property>
    </configuration>
---
apiVersion: v1
kind: Secret
metadata:
  name: hive-db
  namespace: smartcity
stringData:
  database_host: trino-db
  database_name: trino
  database_password: trino
  database_port: "5432"
  database_user: trino
---
apiVersion: v1
kind: Secret
metadata:
  name: trino-catalog
  namespace: smartcity
stringData:
  hive.properties: |
    connector.name=hive-hadoop2
    hive.metastore.uri=thrift://hive-metastore:9083
    hive.s3.endpoint=http://s3.data.local
    hive.s3.signer-type=AWSS3V4SignerType
    hive.s3.path-style-access=false
    hive.s3.staging-directory=/tmp
    hive.non-managed-table-writes-enabled=true
    hive.non-managed-table-creates-enabled=true
    hive.s3.ssl.enabled=false
    hive.s3.sse.enabled=false
    hive.allow-drop-table=true
    hive.parquet.use-column-names=true
    hive.orc.use-column-names=true
    hive.recursive-directories=true
  jmx.properties: |
    connector.name=jmx
  postgresql-lpr.properties: |
    connector.name=postgresql
    connection-url=jdbc:postgresql://smartcity-db-service:5432/pgdb
    connection-user=dbadmin
    connection-password=dbpassword
---
apiVersion: v1
kind: Secret
metadata:
  name: trino-config
  namespace: smartcity
stringData:
  config-coordinator.properties: |-
    coordinator=true
    node-scheduler.include-coordinator=false
    http-server.http.port=8080
    discovery-server.enabled=true
    discovery.uri=http://localhost:8080
  config-worker.properties: |-
    coordinator=false
    http-server.http.port=8080
    discovery.uri=http://trino-service:8080
  jvm.config: |-
    -server
    -Xmx16G
    -XX:-UseBiasedLocking
    -XX:+UseG1GC
    -XX:G1HeapRegionSize=32M
    -XX:+ExplicitGCInvokesConcurrent
    -XX:+ExitOnOutOfMemoryError
    -XX:+HeapDumpOnOutOfMemoryError
    -XX:ReservedCodeCacheSize=512M
    -XX:PerMethodRecompilationCutoff=10000
    -XX:PerBytecodeRecompilationCutoff=10000
    -Djdk.attach.allowAttachSelf=true
    -Djdk.nio.maxCachedBufferSize=2000000
  log.properties: io.trino=INFO
  node.properties: |-
    node.environment=trino
    node.data-dir=/tmp/data/trino
---
apiVersion: v1
kind: Secret
metadata:
  annotations:
    template.openshift.io/expose-password: '{.data[''database-password'']}'
    template.openshift.io/expose-username: '{.data[''database-user'']}'
  labels:
    app: trino
  name: trino-db-secret
  namespace: smartcity
stringData:
  database-name: trino
  database-password: trino
  database-user: trino
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hive
    hive: metastore
  name: hive-metastore
  namespace: smartcity
spec:
  ports:
  - name: meta
    port: 9083
    targetPort: meta
  - name: metrics
    port: 8082
  selector:
    app: hive
    hive: metastore
  sessionAffinity: None
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    template.openshift.io/expose-uri: postgres://{.spec.clusterIP}:{.spec.ports[?(.name=="postgresql")].port}
  labels:
    app: trino
  name: trino-db
  namespace: smartcity
spec:
  ports:
  - name: postgresql
    port: 5432
    protocol: TCP
    targetPort: 5432
  selector:
    deployment: trino-db
---
apiVersion: v1
kind: Service
metadata:
  labels:
    instance: trino
    role: trino-coordinator
  name: trino-service
  namespace: smartcity
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    instance: trino
    role: trino-coordinator
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    volume.beta.kubernetes.io/storage-class: ocs-storagecluster-ceph-rbd
  labels:
    app: trino
  name: trino-db-volume
  namespace: smartcity
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    instance: trino
    role: trino-coordinator
  name: trino-coordinator
  namespace: smartcity
spec:
  selector:
    matchLabels:
      instance: trino
      role: trino-coordinator
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        instance: trino
        role: trino-coordinator
    spec:
      containers:
      - args:
        - --config=/etc/trino/config-coordinator.properties
        command:
        - /usr/lib/trino/bin/run-trino
        env:
        - name: POD_ID
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.uid
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              key: AWS_ACCESS_KEY_ID
              name: secor-obc
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              key: AWS_SECRET_ACCESS_KEY
              name: secor-obc
        - name: TRINO_HISTORY_FILE
          value: /tmp/.trino_history
        image: trinodb/trino:403
        name: trino-coordinator
        ports:
        - containerPort: 8080
          protocol: TCP
        resources:
          limits:
            cpu: "1"
            memory: 4Gi
          requests:
            cpu: 300m
            memory: 4Gi
        volumeMounts:
        - mountPath: /etc/trino
          name: trino-config-volume
        - mountPath: /etc/trino/catalog
          name: trino-catalogs-volume
      volumes:
      - name: trino-config-volume
        secret:
          defaultMode: 420
          items:
          - key: config-coordinator.properties
            path: config-coordinator.properties
          - key: jvm.config
            path: jvm.config
          - key: log.properties
            path: log.properties
          - key: node.properties
            path: node.properties
          secretName: trino-config
      - name: trino-catalogs-volume
        secret:
          defaultMode: 420
          items:
          - key: hive.properties
            path: hive.properties
          - key: postgresql-lpr.properties
            path: postgresql-lpr.properties
          - key: jmx.properties
            path: jmx.properties
          secretName: trino-catalog
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    template.alpha.openshift.io/wait-for-ready: "true"
  labels:
    app: trino
  name: trino-db
  namespace: smartcity
spec:
  selector:
    matchLabels:
      deployment: trino-db
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        deployment: trino-db
    spec:
      containers:
      - env:
        - name: POSTGRESQL_USER
          valueFrom:
            secretKeyRef:
              key: database-user
              name: trino-db-secret
        - name: POSTGRESQL_PASSWORD
          valueFrom:
            secretKeyRef:
              key: database-password
              name: trino-db-secret
        - name: POSTGRESQL_DATABASE
          valueFrom:
            secretKeyRef:
              key: database-name
              name: trino-db-secret
        image: registry.redhat.io/rhel8/postgresql-12:latest
        imagePullPolicy: IfNotPresent
        livenessProbe:
          exec:
            command:
            - /usr/libexec/check-container
            - --live
        name: postgresql
        ports:
        - containerPort: 5432
          protocol: TCP
        readinessProbe:
          exec:
            command:
            - /usr/libexec/check-container
        resources:
          limits:
            cpu: 1
            memory: 2Gi
          requests:
            cpu: 300m
            memory: 1Gi
        terminationMessagePath: /dev/termination-log
        volumeMounts:
        - mountPath: /var/lib/pgsql/data
          name: postgresql-data
      volumes:
      - name: postgresql-data
        persistentVolumeClaim:
          claimName: trino-db-volume
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    instance: trino
    role: trino-worker
  name: trino-worker
  namespace: smartcity
spec:
  replicas: 3
  selector:
    matchLabels:
      instance: trino
      role: trino-worker
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        instance: trino
        role: trino-worker
    spec:
      containers:
      - args:
        - --config=/etc/trino/config-worker.properties
        command:
        - /usr/lib/trino/bin/run-trino
        env:
        - name: POD_ID
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.uid
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              key: AWS_ACCESS_KEY_ID
              name: secor-obc
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              key: AWS_SECRET_ACCESS_KEY
              name: secor-obc
        image: trinodb/trino:403
        name: trino-worker
        ports:
        - containerPort: 8080
          protocol: TCP
        resources:
          limits:
            cpu: "1"
            memory: 4Gi
          requests:
            cpu: 300m
            memory: 4Gi
        volumeMounts:
        - mountPath: /etc/trino
          name: trino-config-volume
        - mountPath: /etc/trino/catalog
          name: trino-catalogs-volume
      volumes:
      - name: trino-config-volume
        secret:
          defaultMode: 420
          items:
          - key: config-worker.properties
            path: config-worker.properties
          - key: jvm.config
            path: jvm.config
          - key: log.properties
            path: log.properties
          - key: node.properties
            path: node.properties
          secretName: trino-config
      - name: trino-catalogs-volume
        secret:
          defaultMode: 420
          items:
          - key: hive.properties
            path: hive.properties
          - key: postgresql-lpr.properties
            path: postgresql-lpr.properties
          - key: jmx.properties
            path: jmx.properties
          secretName: trino-catalog
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: hive
    hive: metastore
  name: hive-metastore
  namespace: smartcity
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hive
      hive: metastore
  serviceName: hive-metastore
  template:
    metadata:
      labels:
        app: hive
        hive: metastore
    spec:
      containers:
      - args:
        - /opt/hive/bin/hive
        - --service
        - metastore
        command:
        - /hive-scripts/entrypoint.sh
        env:
        - name: HIVE_LOGLEVEL
          value: INFO
        - name: S3_ENDPOINT
          value: s3.data.local
        - name: S3_ENDPOINT_URL_PREFIX
          value: http://
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              key: AWS_ACCESS_KEY_ID
              name: secor-obc
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              key: AWS_SECRET_ACCESS_KEY
              name: secor-obc
        - name: S3_BUCKET_NAME
          valueFrom:
            configMapKeyRef:
              key: BUCKET_NAME
              name: secor-obc
        - name: S3_DATA_DIR
          value: data
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              key: database-user
              name: trino-db-secret
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              key: database-password
              name: trino-db-secret
        - name: DATABASE_HOST
          value: trino-db
        - name: DATABASE_PORT
          value: "5432"
        - name: DATABASE_NAME
          value: trino
        - name: DATABASE_SSLMODE
          value: allow
        - name: MY_MEM_REQUEST
          valueFrom:
            resourceFieldRef:
              containerName: metastore
              resource: requests.memory
        - name: MY_MEM_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: metastore
              resource: limits.memory
        image: quay.io/cloudservices/ubi-hive:2.3.3-002
        imagePullPolicy: Always
        livenessProbe:
          failureThreshold: 3
          initialDelaySeconds: 180
          periodSeconds: 10
          successThreshold: 1
          tcpSocket:
            port: 9083
          timeoutSeconds: 5
        name: metastore
        ports:
        - containerPort: 9083
          name: meta
        - containerPort: 8082
          name: metrics
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 8082
            scheme: HTTP
          initialDelaySeconds: 180
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: 1
            memory: 1Gi
          requests:
            cpu: 500m
            memory: 512Mi
        volumeMounts:
        - mountPath: /hive-config
          name: hive-config
        - mountPath: /hive-scripts
          name: hive-scripts
        - mountPath: /opt/jmx_exporter/config
          name: hive-jmx-config
        - mountPath: /var/lib/hive
          name: hive-metastore-db-data
        - mountPath: /hadoop/dfs/name
          name: namenode-empty
        - mountPath: /hadoop/dfs/data
          name: datanode-empty
        - mountPath: /opt/hadoop/logs
          name: hadoop-logs
        - mountPath: /hadoop-config
          name: hadoop-config
      volumes:
      - configMap:
          name: hive-config
        name: hive-config
      - configMap:
          defaultMode: 509
          name: hive-scripts
        name: hive-scripts
      - configMap:
          name: hive-jmx-config
        name: hive-jmx-config
      - emptyDir: {}
        name: namenode-empty
      - emptyDir: {}
        name: datanode-empty
      - emptyDir: {}
        name: hadoop-logs
      - emptyDir: {}
        name: hive-metastore-db-data
      - name: hadoop-config
        secret:
          defaultMode: 420
          secretName: hadoop-config
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    instance: trino
    role: trino-coordinator
  name: trino-route
  namespace: smartcity
spec:
  port:
    targetPort: 8080
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  to:
    kind: Service
    name: trino-service
  wildcardPolicy: None
