= Smart Cities Workshop Deployment Guide

== Requirements

* A working OpenShift environment. Last tested version: 4.7
* Cluster-admin access to OpenShift, or ability to deploy operators.
* OpenShift Data Foundation (ODF). Last tested version: 4.7
* A running RGW instance in ODF. External Route to the RGW must be exposed with SSL enabled for Grafana dashboards to work properly.
* The `oc` client.
* Optional: the `jq` tool (JSON patcher).

== OpenShift/ODF Patch

To circumvent some issues related to bucket access-style (DNS vs Path), we have to make some changes. 

IMPORTANT: All the files used in the following commands are in the `ocp-odf-patch` folder.

=== Deploy CoreDNS into the `openshift-storage` namespace:

.Create the odf-coredns ConfigMap
[source,bash]
----
oc apply -f cm_coredns.yaml
----

.Create the odf-coredns Deployment
[source,bash]
----
oc apply -f dp_coredns.yaml
----

.Create the odf-coredns Service
[source,bash]
----
oc apply -f svc_coredns.yaml
----

=== Patch the `dns.operator/default` object from OpenShift

Here we are adding a new zone, `data.local` to our environment.

.Patching command
[source,bash]
----
oc patch dns.operator/default --type=merge --patch '{"spec":{"servers":[{"forwardPlugin":{"upstreams":["'$(oc get -n openshift-storage svc | grep dns | awk '{print $3}')':5353"]},"name":"rook-dns","zones":["data.local"]}]}}'
----

=== Add the new endpoint to the RGW zone configuration

The RGW must know about this new zone it will serve from.

* If not already done, deploy the Ceph toolbox

[source,bash]
----
oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch  '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'
----

* Add the endpoint 

IMPORTANT: The next command does everything in one step. If you want to understand what's going on (or just don't trust those long commands...), detailed instructions are available in the next section.

.One line command
[source,bash]
----
oc exec -n openshift-storage deploy/rook-ceph-tools -- bash -c "radosgw-admin zonegroup get --rgw-zonegroup=ocs-storagecluster-cephobjectstore > /tmp/config.json && sed -i 's/\"hostnames\": \[],/\"hostnames\": \[\"s3\.data\.local\"],/' /tmp/config.json && radosgw-admin zonegroup set --rgw-zonegroup=ocs-storagecluster-cephobjectstore --infile=/tmp/config.json"
----

=== Manual steps to add the endpoint (Optional)

*** Get the current config

[source,bash]
----
echo $(oc exec -n openshift-storage deploy/rook-ceph-tools -- radosgw-admin zonegroup get --rgw-zonegroup=ocs-storagecluster-cephobjectstore) > config.json
----

** Edit the file config.json

In the file `config.json` you obtained, replace the first occurence of `"hostnames": [],` by `"hostnames": ["s3.data.local"],`.

You can also use jq to do that:
`jq '.hostnames = ["s3.data.local"]' config.json > tmp.json && mv tmp.json config.json` (the complicated part with tmp.json is because json cannot edit in place...).

** Upload the modified file to the toolbox

[source,bash]
----
oc rsync . $(oc get pods -n openshift-storage | grep rook-ceph-tools | grep Running | awk '{print $1}'):/tmp --exclude=* --include=config.json --no-perms
----

** Apply the new configuration

[source,bash]
----
oc exec -n openshift-storage deploy/rook-ceph-tools -- radosgw-admin zonegroup set --rgw-zonegroup=ocs-storagecluster-cephobjectstore --infile=/tmp/config.json
----

== Namespace

Create an OpenShift project/namespace to deploy the environment. In this documentation we'll use `smartcity`.

[source,bash]
----
oc new-project smartcity
----

TIP: If you did not use `smartcity` as the name of your project, don't forget to change it in the commands or the config files used for the deployment.

== Operators

If not already available, deploy the following operators from OperatorHub:

* Red Hat AMQ Streams (all namespaces)
* Grafana Operator (`smartcity` namespace only)
* Presto operator (`smartcity namespace only`)

== Deployment

From the `deploy` folder and subfolders, create the OpenShift resources in this order.

.Creating a resource
[source,bash]
----
oc apply -f file.yaml
----

=== Base elements

We will need a few base elements or helpers to start with. You can edit the Secret or ConfigMap files if you want change the default values. They contain the environment values that will be used by the rest of the deployments.

* `base/secret_postgresql.yaml`: Secrets to deploy the PostgreSQL database
* `base/dc_postgresql.yaml`: Deployment of the PostgreSQL helper database
* `base/service_postgresql.yaml`: Service for PostgreSQL helper database

=== Kafka

We will need two different Kafka instances. One will simulate the "Edges", the toll station, the other one the "Core". We will also create the different topics that are needed, as well as the Kafka Mirror Maker to replicate the topics from the Edge to the Core.

* `kafka/edge.yaml`: Edge Kafka instance
* `kafka/core.yaml`: Core Kafka instance
* `kafka/edge-topic.yaml`: Edge topic
* `kafka/core-topic.yaml`: Core topic
* `kafka/mirror-maker.yaml`: Mirror maker
* `kafka/edge-kafdrop.yaml`: Optional! Kafdrop is a UI interface to your Kafka cluster (to inspect messages)
* `kafka/core-kafdrop.yaml`: Optional! Kafdrop is a UI interface to your Kafka cluster (to inspect messages)

=== Load Generator

This is the component that injects car images into the pipeline.

* `generator/obc_dataset_generator.yaml`: Bucket to store the images dataset
* `generator/is_generator.yaml`: ImageStream for the load generator
* `generator/bc_generator.yaml`: BuildConfiguration to create the load generator image
* `generator/dc_generator.yaml`: Deployment Configuration for the load generator


=== LPR Service

This component presents an API that you can query with an image and returns the infered licence plate number.

* `lpr_service/is_lpr_service.yaml`: ImageStream for the LPR service
* `lpr_service/bc_lpr_service.yaml`: BuildConfiguration for the LPR service
* `lpr_service/dc_lpr_service.yaml`: Deployment Configuration for the LPR service
* `lpr_service/svc_lpr_service.yaml`: Service to access the LPR service


=== Events Service

This is the component that runs in the Core and listens to incoming Kafka events to write them into a PostgreSQL database so that they can be queried to create the dashboards.

* `events_service/is_events_service.yaml`: ImageStream for the event service
* `events_service/bc_events_service.yaml`: BuildConfiguration for the event service
* `events_service/dc_events_service.yaml`: Deployment Configuration for the event service

=== Dataset

Retrieve the information for the dataset bucket created previously and upload the images.

[source,bash]
----
export AWS_ACCESS_KEY_ID=$(oc get secret/generator-dataset -o yaml | grep " AWS_ACCESS_KEY_ID" | awk '{ print $2 }' - | base64 -d)
export AWS_SECRET_ACCESS_KEY=$(oc get secret/generator-dataset -o yaml | grep " AWS_SECRET_ACCESS_KEY" | awk '{ print $2 }' - | base64 -d)
export RGW_ROUTE=https://$(oc get routes -n openshift-storage | grep rgw | awk '{ print $2 }')
export BUCKET=$(oc get cm/generator-dataset -o yaml | grep " BUCKET_NAME:" | awk '{ print $2 }' -)
aws --endpoint-url $RGW_ROUTE s3 cp --recursive ../source/dataset/images s3://$BUCKET/images
----


